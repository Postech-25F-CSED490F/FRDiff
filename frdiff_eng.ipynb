{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0906f4",
   "metadata": {},
   "source": [
    "# FRDiff tutorial\n",
    "This Jupyter notebook is based on “FRDiff: Feature Reuse for Universal Training-free Acceleration of Diffusion Models [ECCV24]” and aims to provide a detailed understanding and implementation of Diffusion Caching techniques.\n",
    "\n",
    "The sequence is as follows:\n",
    "- Analysis of temporal redundancy in diffusion models.\n",
    "- Implementation of Feature Reuse.\n",
    "- Implementation of Score Mixing.\n",
    "- Performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029d1e6",
   "metadata": {},
   "source": [
    "## Install Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do this on python 3.10\n",
    "You can create 3.10 kernel for ipynb by using this command on your shell\n",
    "\n",
    "conda create -n py310 python=3.10\n",
    "conda activate py310\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name py310 --display-name \"Python 3.10\"\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade diffusers==0.33.1\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac2735",
   "metadata": {},
   "source": [
    "## Standard Inference\n",
    "\n",
    "First, we perform inference with a standard diffusion model without applying any compression or acceleration techniques.\n",
    "\n",
    "In this example, the model used is SDXL (Stable Diffusion XL), and DDIM is used as the solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c658b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "# Stable Diffusion 1.4\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "# SDXL\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                                 variant=\"fp16\", torch_dtype=torch.float16,\n",
    "                                                 use_safetensors=True)\n",
    "\n",
    "pipe.to(f'cuda')\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30facc",
   "metadata": {},
   "source": [
    "We set the denoising process to 50 steps and configure the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c257f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "# Sampling Step\n",
    "num_steps = 50\n",
    "\n",
    "# Set your prompt !\n",
    "prompt = \"a photo of an astronaut riding a horse on mars, trending on artstation, high quality, detailed, cinematic lighting\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2e94c",
   "metadata": {},
   "source": [
    "### DDIM Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30216052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "t1 = time.time()\n",
    "image_ddim = pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]\n",
    "t2 = time.time()\n",
    "\n",
    "time_ddim = t2 - t1\n",
    "print(f\"DDIM Sampling Time: {time_ddim:.2f} seconds\")\n",
    "image_ddim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880352f",
   "metadata": {},
   "source": [
    "We can confirm that the image is generated correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a695bf4",
   "metadata": {},
   "source": [
    "### Temporal Redundancy Analysis\n",
    "\n",
    "SDXL uses a UNet model, which is primarily composed of two types of blocks.\n",
    "\n",
    "- ResnetBlock2D\n",
    "- Transformer2DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7728152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from typing import Union, Optional, Dict, Callable, List, Any, Tuple\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    StableDiffusionXLPipelineOutput,\n",
    "    retrieve_timesteps,\n",
    "    rescale_noise_cfg,\n",
    "    XLA_AVAILABLE,\n",
    ")\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import (\n",
    "    StableDiffusionPipeline, StableDiffusionPipelineOutput\n",
    ")\n",
    "\n",
    "from diffusers.models.resnet import ResnetBlock2D\n",
    "from diffusers.models.upsampling import Upsample2D\n",
    "from diffusers.models.downsampling import Downsample2D\n",
    "from diffusers.models.transformers.transformer_2d import Transformer2DModel, Transformer2DModelOutput\n",
    "from diffusers.utils import is_torch_version, USE_PEFT_BACKEND, deprecate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n , m in pipe.unet.named_modules():\n",
    "    if isinstance(m, ResnetBlock2D):\n",
    "        print(m.__class__.__name__, n)\n",
    "    elif isinstance(m, Transformer2DModel):\n",
    "        print(m.__class__.__name__, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d77c69",
   "metadata": {},
   "source": [
    "As we can see above, the structure alternates between ResBlock and TransformerBlock.\n",
    "\n",
    "To analyze temporal redundancy, we will first focus only on the ResBlocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9477112",
   "metadata": {},
   "source": [
    "### ResnetBlock2D\n",
    "\n",
    "Below is the original implementation of the forward path of SDXL’s ResBlock.\n",
    "\n",
    "You can also find the original code at\n",
    "'https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0521115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "    if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "        deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "        deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "    hidden_states = input_tensor\n",
    "\n",
    "    hidden_states = self.norm1(hidden_states)\n",
    "    hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "    if self.upsample is not None:\n",
    "        # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "        if hidden_states.shape[0] >= 64:\n",
    "            input_tensor = input_tensor.contiguous()\n",
    "            hidden_states = hidden_states.contiguous()\n",
    "        input_tensor = self.upsample(input_tensor)\n",
    "        hidden_states = self.upsample(hidden_states)\n",
    "    elif self.downsample is not None:\n",
    "        input_tensor = self.downsample(input_tensor)\n",
    "        hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "    hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "    if self.time_emb_proj is not None:\n",
    "        if not self.skip_time_act:\n",
    "            temb = self.nonlinearity(temb)\n",
    "        temb = self.time_emb_proj(temb)[:, :, None, None]\n",
    "\n",
    "    if self.time_embedding_norm == \"default\":\n",
    "        if temb is not None:\n",
    "            hidden_states = hidden_states + temb\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "    elif self.time_embedding_norm == \"scale_shift\":\n",
    "        if temb is None:\n",
    "            raise ValueError(\n",
    "                f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n",
    "            )\n",
    "        time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "        hidden_states = hidden_states * (1 + time_scale) + time_shift\n",
    "    else:\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "    hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "    hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "    if self.conv_shortcut is not None:\n",
    "        input_tensor = self.conv_shortcut(input_tensor.contiguous())\n",
    "\n",
    "    output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9200177",
   "metadata": {},
   "source": [
    "As we can see from the code, the computation follows the structure below:\n",
    "\n",
    "1. res <- x\n",
    "2. x <- Conv1(x)\n",
    "3. x <- Conv2 ( NonLinear( x+ t ) )\n",
    "4. x <- res + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b32799",
   "metadata": {},
   "source": [
    "We now insert logging code for temporal redundancy analysis.\n",
    "\n",
    "The sections wrapped with \\#\\#\\#\\# MODIFED \\#\\#\\#\\# indicate the modified parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieved from Original repo : 'https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py\n",
    "class SkipResnetBlock2D(ResnetBlock2D):\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ############## MODIFIED ###############\n",
    "        self.res_memory.append(hidden_states.cpu())\n",
    "        ############## MODIFIED ###############\n",
    "        #######################################\n",
    "\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        if self.time_emb_proj is not None:\n",
    "            if not self.skip_time_act:\n",
    "                temb = self.nonlinearity(temb)\n",
    "            temb = self.time_emb_proj(temb)[:, :, None, None]\n",
    "\n",
    "        if self.time_embedding_norm == \"default\":\n",
    "            if temb is not None:\n",
    "                hidden_states = hidden_states + temb\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "        elif self.time_embedding_norm == \"scale_shift\":\n",
    "            if temb is None:\n",
    "                raise ValueError(\n",
    "                    f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n",
    "                )\n",
    "            time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "            hidden_states = hidden_states * (1 + time_scale) + time_shift\n",
    "        else:\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            input_tensor = self.conv_shortcut(input_tensor.contiguous())\n",
    "\n",
    "        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d54dfd",
   "metadata": {},
   "source": [
    "We select the layers where we will insert the logging code, and replace the `ResnetBlock2D` class with the subclass `SkipResnetBlock2D` to inject the logging code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = ['down_blocks.0.resnets.0','mid_block.resnets.0']\n",
    "\n",
    "# Patch Classes\n",
    "for n, m in pipe.unet.named_modules() :\n",
    "    #print(n)\n",
    "    if n in target_layers:\n",
    "        #print(n)\n",
    "        print(f'Convert {type(m)} to SkipResnetBlock2D')\n",
    "        m.__class__ = SkipResnetBlock2D\n",
    "        m.res_memory = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534ecc6",
   "metadata": {},
   "source": [
    "We generate an image and retrieve the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940121cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49667f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "\n",
    "for n,m in pipe.unet.named_modules():\n",
    "    if isinstance(m, SkipResnetBlock2D):\n",
    "        print(f'Collecting {n} logs')\n",
    "        logs.append(m.res_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b8b93",
   "metadata": {},
   "source": [
    "Now, using the retrieved logs, we analyze temporal redundancy.\n",
    "\n",
    "First, by visualizing the saved feature tensor for one channel at each timestep, we can observe how the features change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log = logs[0]\n",
    "channel_num = 1\n",
    "\n",
    "num_features = len(log)\n",
    "cols = 10\n",
    "rows = (num_features + cols - 1) // cols  # 올림\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "\n",
    "for idx, t_feature in enumerate(log):\n",
    "    r = idx // cols\n",
    "    c = idx % cols\n",
    "    t_img = t_feature[0][channel_num].cpu().numpy()\n",
    "    axes[r, c].imshow(t_img, cmap='gray')\n",
    "    axes[r, c].axis('off')\n",
    "    axes[r, c].set_title(f\"t={idx}\")\n",
    "\n",
    "# 남는 subplot은 안보이게\n",
    "for idx in range(num_features, rows * cols):\n",
    "    r = idx // cols\n",
    "    c = idx % cols\n",
    "    axes[r, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdcf3c",
   "metadata": {},
   "source": [
    "As we can see above, these features exhibit only very small changes across consecutive timesteps.\n",
    "\n",
    "We can also analyze this numerically as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for layer_log in logs :\n",
    "    errs = []\n",
    "    for t in range(1, len(layer_log)):\n",
    "        err = (layer_log[t] - layer_log[t-1]).abs().mean().item()\n",
    "        errs.append(err)\n",
    "\n",
    "    \n",
    "    plt.plot(errs)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.title(\"Temporal Feature Differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2358db6",
   "metadata": {},
   "source": [
    "Similarly, we can confirm that it has a small numerical error. This small amount of change suggests the possibility of significantly reducing computation by reusing once-computed features in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c3249",
   "metadata": {},
   "source": [
    "## FRDiff\n",
    "\n",
    "Now, we will cover the implementation of our work, FRDiff. FRDiff consists of three components:\n",
    "- Feature Reuse\n",
    "- Score Mixing\n",
    "- Auto-FR\n",
    "\n",
    "Although it is composed of these three, in this tutorial we will implement only Feature-Reuse and Score-Mixing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc9f3f",
   "metadata": {},
   "source": [
    "### 1. Feature-Reuse\n",
    "\n",
    "Feature Reuse has the following formulation.\n",
    "\n",
    "First, for all blocks with a residual structure such as `ResBlock`or `TransformerBlock`,  \n",
    "$ y \\leftarrow x + \\mathcal{F}(x,t)$,  \n",
    "we decompose the non-residual part $\\mathcal{F}(x,t)$ into the following form:\n",
    "\n",
    "$\\mathcal{F}(x_t,t) \\leftarrow f( \\mathcal{S}(x_t), t)$\n",
    "\n",
    "Here, $\\mathcal{S}(x_t)$ is the part that does not compute information regarding the time step, and we refer to it as the *Spatial Block*.\n",
    "\n",
    "For example, in the previous ResBlock structure:\n",
    "1.\tres <- x\n",
    "2.\tx <- Conv1(x)\n",
    "3.\tx <- Conv2 ( NonLinear( x + t ) )\n",
    "4.\tx <- res + x\n",
    "\n",
    "the step 2. x <- Conv1(x) corresponds to the *Spatial Block*.\n",
    "\n",
    "---\n",
    "\n",
    "Next, we define the *Keyframe set* $\\mathcal{K}$ (i.e, {10,20,30,40,50}), which specifies the points where Feature Caching is performed.\n",
    "\n",
    "When the current denoising step $t$ satisfies $t \\in \\mathcal{K}$, caching is performed.  \n",
    "$if\\ \\ t \\in \\mathcal{K} :$  \n",
    "$\\quad M \\leftarrow S(x_t)$  \n",
    "$\\quad y \\leftarrow f(M, t) + x_t$\n",
    "\n",
    "---\n",
    "Below is the implementation in the ResBlock accordingly.\n",
    "The parts wrapped with \\#\\#\\#\\# MODIFED \\#\\#\\#\\# indicate the modified sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c978cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieved from Original repo : 'https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py\n",
    "class SkipResnetBlock2D(ResnetBlock2D):\n",
    "    \n",
    "    ### MODIFIED #####\n",
    "    # cleaner\n",
    "    def update(self):\n",
    "        self.layer_memory = None\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ############## MODIFIED ###############\n",
    "        # Original Forward\n",
    "        # hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        # Our Feature Reuse\n",
    "        if self.GLOBAL_BUFFER.current_step in self.GLOBAL_BUFFER.skip_cnt: # if t \\in K\n",
    "            self.layer_memory = self.conv1(hidden_states)\n",
    "        hidden_states = self.layer_memory\n",
    "        ############## MODIFIED ###############\n",
    "        #######################################\n",
    "\n",
    "\n",
    "        if self.time_emb_proj is not None:\n",
    "            if not self.skip_time_act:\n",
    "                temb = self.nonlinearity(temb)\n",
    "            temb = self.time_emb_proj(temb)[:, :, None, None]\n",
    "\n",
    "        if self.time_embedding_norm == \"default\":\n",
    "            if temb is not None:\n",
    "                hidden_states = hidden_states + temb\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "        elif self.time_embedding_norm == \"scale_shift\":\n",
    "            if temb is None:\n",
    "                raise ValueError(\n",
    "                    f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n",
    "                )\n",
    "            time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "            hidden_states = hidden_states * (1 + time_scale) + time_shift\n",
    "        else:\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            input_tensor = self.conv_shortcut(input_tensor.contiguous())\n",
    "\n",
    "        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901fd98",
   "metadata": {},
   "source": [
    "The following is the implementation in the Transformer2D Block.\n",
    "Since the Transformer block does not use timestep information anywhere inside the block, we can simply separate the residual part and reuse it.\n",
    "\n",
    "Thus, it is implemented with the following structure. (_forward is the original forward function.)\n",
    "\n",
    "```\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states, ...):\n",
    "\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Feature Reuse\n",
    "        if t in KEYFRAME_SET:\n",
    "            self.layer_memory = self._forward(hidden_states,  ... )[0] - residual\n",
    "            \n",
    "        output = self.layer_memory + residual\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7280291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipTransformer2DModel(Transformer2DModel):\n",
    "    def update(self):\n",
    "        self.layer_memory = None\n",
    "\n",
    "    #### MODIFIED #####\n",
    "    #override forward function.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        timestep: Optional[torch.LongTensor] = None,\n",
    "        added_cond_kwargs: Dict[str, torch.Tensor] = None,\n",
    "        class_labels: Optional[torch.LongTensor] = None,\n",
    "        cross_attention_kwargs: Dict[str, Any] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ):\n",
    "        assert self.is_input_continuous is True\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Feature Reuse\n",
    "        if self.GLOBAL_BUFFER.current_step in self.GLOBAL_BUFFER.skip_cnt:\n",
    "\n",
    "            self.layer_memory = self._forward(hidden_states, encoder_hidden_states,\n",
    "                                            timestep, added_cond_kwargs, class_labels,\n",
    "                                            cross_attention_kwargs, attention_mask,\n",
    "                                            encoder_attention_mask, return_dict)[0] - residual\n",
    "            # We only cache the non-residual output part. So we subtract the residual from the output.\n",
    "\n",
    "\n",
    "        output = self.layer_memory + residual\n",
    "\n",
    "        if not return_dict:\n",
    "            return (output,)\n",
    "\n",
    "        return Transformer2DModelOutput(sample=output)\n",
    "\n",
    "    #### Original Forward ####\n",
    "    def _forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        timestep: Optional[torch.LongTensor] = None,\n",
    "        added_cond_kwargs: Dict[str, torch.Tensor] = None,\n",
    "        class_labels: Optional[torch.LongTensor] = None,\n",
    "        cross_attention_kwargs: Dict[str, Any] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The [`Transformer2DModel`] forward method.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (`torch.LongTensor` of shape `(batch size, num latent pixels)` if discrete, `torch.Tensor` of shape `(batch size, channel, height, width)` if continuous):\n",
    "                Input `hidden_states`.\n",
    "            encoder_hidden_states ( `torch.Tensor` of shape `(batch size, sequence len, embed dims)`, *optional*):\n",
    "                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to\n",
    "                self-attention.\n",
    "            timestep ( `torch.LongTensor`, *optional*):\n",
    "                Used to indicate denoising step. Optional timestep to be applied as an embedding in `AdaLayerNorm`.\n",
    "            class_labels ( `torch.LongTensor` of shape `(batch size, num classes)`, *optional*):\n",
    "                Used to indicate class labels conditioning. Optional class labels to be applied as an embedding in\n",
    "                `AdaLayerZeroNorm`.\n",
    "            cross_attention_kwargs ( `Dict[str, Any]`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n",
    "            attention_mask ( `torch.Tensor`, *optional*):\n",
    "                An attention mask of shape `(batch, key_tokens)` is applied to `encoder_hidden_states`. If `1` the mask\n",
    "                is kept, otherwise if `0` it is discarded. Mask will be converted into a bias, which adds large\n",
    "                negative values to the attention scores corresponding to \"discard\" tokens.\n",
    "            encoder_attention_mask ( `torch.Tensor`, *optional*):\n",
    "                Cross-attention mask applied to `encoder_hidden_states`. Two formats supported:\n",
    "\n",
    "                    * Mask `(batch, sequence_length)` True = keep, False = discard.\n",
    "                    * Bias `(batch, 1, sequence_length)` 0 = keep, -10000 = discard.\n",
    "\n",
    "                If `ndim == 2`: will be interpreted as a mask, then converted into a bias consistent with the format\n",
    "                above. This bias will be added to the cross-attention scores.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.unets.unet_2d_condition.UNet2DConditionOutput`] instead of a plain\n",
    "                tuple.\n",
    "\n",
    "        Returns:\n",
    "            If `return_dict` is True, an [`~models.transformers.transformer_2d.Transformer2DModelOutput`] is returned,\n",
    "            otherwise a `tuple` where the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "        if cross_attention_kwargs is not None:\n",
    "            if cross_attention_kwargs.get(\"scale\", None) is not None:\n",
    "                logger.warning(\"Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.\")\n",
    "        # ensure attention_mask is a bias, and give it a singleton query_tokens dimension.\n",
    "        #   we may have done this conversion already, e.g. if we came here via UNet2DConditionModel#forward.\n",
    "        #   we can tell by counting dims; if ndim == 2: it's a mask rather than a bias.\n",
    "        # expects mask of shape:\n",
    "        #   [batch, key_tokens]\n",
    "        # adds singleton query_tokens dimension:\n",
    "        #   [batch,                    1, key_tokens]\n",
    "        # this helps to broadcast it as a bias over attention scores, which will be in one of the following shapes:\n",
    "        #   [batch,  heads, query_tokens, key_tokens] (e.g. torch sdp attn)\n",
    "        #   [batch * heads, query_tokens, key_tokens] (e.g. xformers or classic attn)\n",
    "        if attention_mask is not None and attention_mask.ndim == 2:\n",
    "            # assume that mask is expressed as:\n",
    "            #   (1 = keep,      0 = discard)\n",
    "            # convert mask into a bias that can be added to attention scores:\n",
    "            #       (keep = +0,     discard = -10000.0)\n",
    "            attention_mask = (1 - attention_mask.to(hidden_states.dtype)) * -10000.0\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "        # convert encoder_attention_mask to a bias the same way we do for attention_mask\n",
    "        if encoder_attention_mask is not None and encoder_attention_mask.ndim == 2:\n",
    "            encoder_attention_mask = (1 - encoder_attention_mask.to(hidden_states.dtype)) * -10000.0\n",
    "            encoder_attention_mask = encoder_attention_mask.unsqueeze(1)\n",
    "\n",
    "        # 1. Input\n",
    "        if self.is_input_continuous:\n",
    "            batch_size, _, height, width = hidden_states.shape\n",
    "            residual = hidden_states\n",
    "            hidden_states, inner_dim = self._operate_on_continuous_inputs(hidden_states)\n",
    "        elif self.is_input_vectorized:\n",
    "            hidden_states = self.latent_image_embedding(hidden_states)\n",
    "        elif self.is_input_patches:\n",
    "            height, width = hidden_states.shape[-2] // self.patch_size, hidden_states.shape[-1] // self.patch_size\n",
    "            hidden_states, encoder_hidden_states, timestep, embedded_timestep = self._operate_on_patched_inputs(\n",
    "                hidden_states, encoder_hidden_states, timestep, added_cond_kwargs\n",
    "            )\n",
    "\n",
    "        # 2. Blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            if torch.is_grad_enabled() and self.gradient_checkpointing:\n",
    "                hidden_states = self._gradient_checkpointing_func(\n",
    "                    block,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    timestep,\n",
    "                    cross_attention_kwargs,\n",
    "                    class_labels,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = block(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    timestep=timestep,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    class_labels=class_labels,\n",
    "                )\n",
    "\n",
    "\n",
    "    \n",
    "        # 3. Output\n",
    "        if self.is_input_continuous:\n",
    "            output = self._get_output_for_continuous_inputs(\n",
    "                hidden_states=hidden_states,\n",
    "                residual=residual,\n",
    "                batch_size=batch_size,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                inner_dim=inner_dim,\n",
    "            )\n",
    "        elif self.is_input_vectorized:\n",
    "            output = self._get_output_for_vectorized_inputs(hidden_states)\n",
    "        elif self.is_input_patches:\n",
    "            output = self._get_output_for_patched_inputs(\n",
    "                hidden_states=hidden_states,\n",
    "                timestep=timestep,\n",
    "                class_labels=class_labels,\n",
    "                embedded_timestep=embedded_timestep,\n",
    "                height=height,\n",
    "                width=width,\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            return (output,)\n",
    "\n",
    "        return Transformer2DModelOutput(sample=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9057011",
   "metadata": {},
   "source": [
    "Finally, we modify the Pipeline function as follows to record the current denoising timestep in `GLOBAL_BUFFER.current_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97776cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipStableDiffusionXLPipeline(StableDiffusionXLPipeline):\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        denoising_end: Optional[float] = None,\n",
    "        guidance_scale: float = 5.0,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        ip_adapter_image : Any = None,  # PipelineImageInput\n",
    "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        original_size: Optional[Tuple[int, int]] = None,\n",
    "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        target_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end : Any = None,  # Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        callback = kwargs.pop(\"callback\", None)\n",
    "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "        if callback is not None:\n",
    "            deprecate(\n",
    "                \"callback\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "        if callback_steps is not None:\n",
    "            deprecate(\n",
    "                \"callback_steps\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "\n",
    "        #if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "        #    callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.default_sample_size * self.vae_scale_factor\n",
    "        width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "        original_size = original_size or (height, width)\n",
    "        target_size = target_size or (height, width)\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            prompt_2,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            negative_prompt_2,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            callback_on_step_end_tensor_inputs,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._guidance_rescale = guidance_rescale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._cross_attention_kwargs = cross_attention_kwargs\n",
    "        self._denoising_end = denoising_end\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        lora_scale = (\n",
    "            self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
    "        )\n",
    "\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt=prompt,\n",
    "            prompt_2=prompt_2,\n",
    "            device=device,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_2=negative_prompt_2,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            lora_scale=lora_scale,\n",
    "            clip_skip=self.clip_skip,\n",
    "        )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(\n",
    "            self.scheduler, num_inference_steps, device, timesteps, sigmas\n",
    "        )\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Prepare added time ids & embeddings\n",
    "        add_text_embeds = pooled_prompt_embeds\n",
    "        if self.text_encoder_2 is None:\n",
    "            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
    "        else:\n",
    "            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n",
    "\n",
    "        add_time_ids = self._get_add_time_ids(\n",
    "            original_size,\n",
    "            crops_coords_top_left,\n",
    "            target_size,\n",
    "            dtype=prompt_embeds.dtype,\n",
    "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "        )\n",
    "        if negative_original_size is not None and negative_target_size is not None:\n",
    "            negative_add_time_ids = self._get_add_time_ids(\n",
    "                negative_original_size,\n",
    "                negative_crops_coords_top_left,\n",
    "                negative_target_size,\n",
    "                dtype=prompt_embeds.dtype,\n",
    "                text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "            )\n",
    "        else:\n",
    "            negative_add_time_ids = add_time_ids\n",
    "\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
    "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
    "\n",
    "        prompt_embeds = prompt_embeds.to(device)\n",
    "        add_text_embeds = add_text_embeds.to(device)\n",
    "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
    "\n",
    "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "\n",
    "        # 8.1 Apply denoising_end\n",
    "        if (\n",
    "            self.denoising_end is not None\n",
    "            and isinstance(self.denoising_end, float)\n",
    "            and self.denoising_end > 0\n",
    "            and self.denoising_end < 1\n",
    "        ):\n",
    "            discrete_timestep_cutoff = int(\n",
    "                round(\n",
    "                    self.scheduler.config.num_train_timesteps\n",
    "                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)\n",
    "                )\n",
    "            )\n",
    "            num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n",
    "            timesteps = timesteps[:num_inference_steps]\n",
    "\n",
    "        # 9. Optionally get Guidance Scale Embedding\n",
    "        timestep_cond = None\n",
    "        if self.unet.config.time_cond_proj_dim is not None:\n",
    "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "            timestep_cond = self.get_guidance_scale_embedding(\n",
    "                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
    "            ).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "\n",
    "        ##################################\n",
    "        ########### MODIFIED #############\n",
    "        # Initialize\n",
    "        self.unet.GLOBAL_BUFFER.current_step = -1\n",
    "        ########### MODIFIED #############\n",
    "        ##################################\n",
    "        self._num_timesteps = len(timesteps)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
    "                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "                    added_cond_kwargs[\"image_embeds\"] = image_embeds\n",
    "\n",
    "                ##################################\n",
    "                ########### MODIFIED #############\n",
    "                # Before each U-Net forward pass, we increment the step\n",
    "                self.unet.GLOBAL_BUFFER.current_step += 1\n",
    "                ########### MODIFIED #############\n",
    "                ##################################\n",
    "                noise_pred = self.unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    timestep_cond=timestep_cond,\n",
    "                    cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                    added_cond_kwargs=added_cond_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://huggingface.co/papers/2305.08891\n",
    "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents_dtype = latents.dtype\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "                if latents.dtype != latents_dtype:\n",
    "                    if torch.backends.mps.is_available():\n",
    "                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                        latents = latents.to(latents_dtype)\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    add_text_embeds = callback_outputs.pop(\"add_text_embeds\", add_text_embeds)\n",
    "                    add_time_ids = callback_outputs.pop(\"add_time_ids\", add_time_ids)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "                if XLA_AVAILABLE:\n",
    "                    xm.mark_step()\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # make sure the VAE is in float32 mode, as it overflows in float16\n",
    "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
    "\n",
    "            if needs_upcasting:\n",
    "                self.upcast_vae()\n",
    "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
    "            elif latents.dtype != self.vae.dtype:\n",
    "                if torch.backends.mps.is_available():\n",
    "                    # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                    self.vae = self.vae.to(latents.dtype)\n",
    "\n",
    "            # unscale/denormalize the latents\n",
    "            # denormalize with the mean and std if available and not None\n",
    "            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n",
    "            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n",
    "            if has_latents_mean and has_latents_std:\n",
    "                latents_mean = (\n",
    "                    torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "                )\n",
    "                latents_std = (\n",
    "                    torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "                )\n",
    "                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n",
    "            else:\n",
    "                latents = latents / self.vae.config.scaling_factor\n",
    "\n",
    "            image = self.vae.decode(latents, return_dict=False)[0]\n",
    "\n",
    "            # cast back to fp16 if needed\n",
    "            if needs_upcasting:\n",
    "                self.vae.to(dtype=torch.float16)\n",
    "        else:\n",
    "            image = latents\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # apply watermark if available\n",
    "            if self.watermark is not None:\n",
    "                image = self.watermark.apply_watermark(image)\n",
    "\n",
    "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusionXLPipelineOutput(images=image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567eb261",
   "metadata": {},
   "source": [
    "Finally, we inject all newly modified classes into every class of the diffusion model.\n",
    "\n",
    "At this time, to create a global variable buffer that can be referenced by every layer instance, we create an instance of an empty class and pass it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapping_frdiff(pipe, total_step, skip_interval, bias=0.5, score_mix=True, debug=False):\n",
    "\n",
    "    specials = {\n",
    "    ResnetBlock2D : SkipResnetBlock2D,\n",
    "    Transformer2DModel : SkipTransformer2DModel,\n",
    "    }\n",
    "    global DEBUG\n",
    "    DEBUG=debug\n",
    "\n",
    "    # Set BUFFER\n",
    "    class BUFFER : pass\n",
    "    b = BUFFER()\n",
    "    pipe.GLOBAL_BUFFER = b\n",
    "\n",
    "    # Inject to every layer\n",
    "    for n, m in pipe.unet.named_modules() :\n",
    "        m.module_name = n\n",
    "        m.GLOBAL_BUFFER = b\n",
    "\n",
    "    pipe.unet.GLOBAL_BUFFER.total_step       = total_step\n",
    "    pipe.unet.GLOBAL_BUFFER.skip_interval    = skip_interval\n",
    "    pipe.unet.GLOBAL_BUFFER.bias             = bias\n",
    "    pipe.unet.GLOBAL_BUFFER.score_mix        = score_mix\n",
    "    pipe.unet.GLOBAL_BUFFER.skip_cnt         = list(range(0,total_step,skip_interval))\n",
    "    print(\"[SKIPPING] Buffer Sharing Complete\")\n",
    "\n",
    "    ### Patch Classes\n",
    "    # main\n",
    "    pipe.__class__ = SkipStableDiffusionXLPipeline\n",
    "\n",
    "    # Blocks\n",
    "    for n, m in pipe.unet.named_modules() :\n",
    "        if type(m) in specials.keys():\n",
    "            print(f'Convert {type(m)} to {type(m)}')\n",
    "            m.__class__ = specials[type(m)]\n",
    "\n",
    "            m.update()\n",
    "    print(\"[SKIPPING] Class Patching Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334aebb5",
   "metadata": {},
   "source": [
    "We create a new pipeline and apply the wrapper. As an example, we first generate the case where interval = 5, i.e., the keyframe set is {0,5,10,15, …}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c87f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                                 variant=\"fp16\", torch_dtype=torch.float16,\n",
    "                                                 use_safetensors=True)\n",
    "\n",
    "pipe.to(f'cuda')\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "wrapping_frdiff(pipe, total_step=50, skip_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c40c7",
   "metadata": {},
   "source": [
    "We generate a new image using the pipeline with Feature Reuse applied, and measure the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14107c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "t1 = time.time()\n",
    "image_fr = pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]\n",
    "t2 = time.time()\n",
    "\n",
    "time_fr = t2 - t1\n",
    "print(f\"Feature Reuse Time : {time_fr:.2f} seconds\")\n",
    "image_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5806cf5",
   "metadata": {},
   "source": [
    "We will compare the quality and the time of the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2302a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(image_ddim)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(f\"DDIM\\nTime: {time_ddim:.2f} seconds\")\n",
    "\n",
    "axes[1].imshow(image_fr)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(f\"FRDiff\\nTime: {time_fr:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd61315",
   "metadata": {},
   "source": [
    "Although the generation quality is slightly degraded, we can see that it was generated in a much faster time.\n",
    "\n",
    "By adjusting the keyframe interval, we can control the trade-off between generation quality and generation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7def4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL = 2\n",
    "\n",
    "pipe2 = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                                 variant=\"fp16\", torch_dtype=torch.float16,\n",
    "                                                 use_safetensors=True)\n",
    "\n",
    "pipe2.to(f'cuda')\n",
    "pipe2.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "wrapping_frdiff(pipe2, total_step=50, skip_interval=INTERVAL)\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae506c",
   "metadata": {},
   "source": [
    "### 2. Score Mixing\n",
    "\n",
    "Next is Score Mixing, a technique that improves the degraded generation quality caused by FR by using the noise prediction from the early denoising steps, which primarily contain the low-frequency information of the final image.\n",
    "\n",
    "The original denoising process of a diffusion model is as follows:\n",
    "\n",
    "$ x_{t+1} \\leftarrow \\epsilon_\\theta(x_t, t)$\n",
    "\n",
    "Here, $\\epsilon_\\theta$ denotes the neural network, which is the U-Net in SDXL.\n",
    "\n",
    "The denoising process with Score Mixing applied is expressed as follows:\n",
    "\n",
    "$ if\\quad t \\in \\mathcal{K} :$  \n",
    "$\\quad E \\leftarrow \\epsilon_\\theta(x_t, t)$  \n",
    "$\\quad x_{t+1} \\leftarrow \\lambda_t \\cdot \\epsilon_\\theta(x_t, t) + ( 1 - \\lambda_t) \\cdot E$\n",
    "\n",
    "Since this is applied at the output level, we modify the above StableDiffusionXLPipeline as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipStableDiffusionXLPipeline(StableDiffusionXLPipeline):\n",
    "\n",
    "    def lamb_schedule(self, step_ratio, _t=30, _b=0.5) :\n",
    "        lamb = np.maximum(0.0, np.minimum(1, (_t * (step_ratio - _b) + 2)/4))\n",
    "        lamb = float(lamb)\n",
    "        #lamb = 1 #Skip\n",
    "        #lamb = 0 #Jump\n",
    "        return lamb\n",
    "\n",
    "    def mix_func(self,lamb, e_t, e_jump) :\n",
    "        return lamb*e_t + (1-lamb)*e_jump\n",
    "\n",
    "    def score_mix(self, model_input, t, **kwargs):\n",
    "        total_step      = self.unet.GLOBAL_BUFFER.total_step\n",
    "        current_step    = self.unet.GLOBAL_BUFFER.current_step\n",
    "        skip_cnt        = self.unet.GLOBAL_BUFFER.skip_cnt\n",
    "\n",
    "        # Compute KeyFrame\n",
    "        if current_step in skip_cnt :\n",
    "            self.jump_score = self.unet(model_input, t, **kwargs)[0]\n",
    "            return self.jump_score\n",
    "        else :\n",
    "            step_ratio = current_step/total_step\n",
    "            bias       = self.unet.GLOBAL_BUFFER.bias\n",
    "            lamb       = self.lamb_schedule(step_ratio, _b=bias)\n",
    "\n",
    "            if DEBUG: print(lamb)\n",
    "            if lamb < 1e-10 :\n",
    "                # Reduced NFE\n",
    "                e_t = self.jump_score\n",
    "            else :\n",
    "                # Score Mixing\n",
    "                noise_pred = self.unet(model_input, t, **kwargs)[0]\n",
    "\n",
    "                if type(noise_pred) == list or type(noise_pred) == tuple :\n",
    "                    # Maybe this loop can be little slow..\n",
    "                    e_t = [  self.mix_func(lamb, e_t_i, e_jump_i) for e_t_i , e_jump_i in zip(noise_pred, self.jump_score) ]\n",
    "                else :\n",
    "                    e_t = self.mix_func(lamb, noise_pred, self.jump_score)\n",
    "            return e_t\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        denoising_end: Optional[float] = None,\n",
    "        guidance_scale: float = 5.0,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        ip_adapter_image : Any = None,  # PipelineImageInput\n",
    "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        original_size: Optional[Tuple[int, int]] = None,\n",
    "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        target_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end : Any = None,  # Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        callback = kwargs.pop(\"callback\", None)\n",
    "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "        if callback is not None:\n",
    "            deprecate(\n",
    "                \"callback\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "        if callback_steps is not None:\n",
    "            deprecate(\n",
    "                \"callback_steps\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "\n",
    "        #if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "        #    callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.default_sample_size * self.vae_scale_factor\n",
    "        width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "        original_size = original_size or (height, width)\n",
    "        target_size = target_size or (height, width)\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            prompt_2,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            negative_prompt_2,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            callback_on_step_end_tensor_inputs,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._guidance_rescale = guidance_rescale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._cross_attention_kwargs = cross_attention_kwargs\n",
    "        self._denoising_end = denoising_end\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        lora_scale = (\n",
    "            self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
    "        )\n",
    "\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt=prompt,\n",
    "            prompt_2=prompt_2,\n",
    "            device=device,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_2=negative_prompt_2,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            lora_scale=lora_scale,\n",
    "            clip_skip=self.clip_skip,\n",
    "        )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(\n",
    "            self.scheduler, num_inference_steps, device, timesteps, sigmas\n",
    "        )\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Prepare added time ids & embeddings\n",
    "        add_text_embeds = pooled_prompt_embeds\n",
    "        if self.text_encoder_2 is None:\n",
    "            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
    "        else:\n",
    "            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n",
    "\n",
    "        add_time_ids = self._get_add_time_ids(\n",
    "            original_size,\n",
    "            crops_coords_top_left,\n",
    "            target_size,\n",
    "            dtype=prompt_embeds.dtype,\n",
    "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "        )\n",
    "        if negative_original_size is not None and negative_target_size is not None:\n",
    "            negative_add_time_ids = self._get_add_time_ids(\n",
    "                negative_original_size,\n",
    "                negative_crops_coords_top_left,\n",
    "                negative_target_size,\n",
    "                dtype=prompt_embeds.dtype,\n",
    "                text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "            )\n",
    "        else:\n",
    "            negative_add_time_ids = add_time_ids\n",
    "\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
    "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
    "\n",
    "        prompt_embeds = prompt_embeds.to(device)\n",
    "        add_text_embeds = add_text_embeds.to(device)\n",
    "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
    "\n",
    "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "\n",
    "        # 8.1 Apply denoising_end\n",
    "        if (\n",
    "            self.denoising_end is not None\n",
    "            and isinstance(self.denoising_end, float)\n",
    "            and self.denoising_end > 0\n",
    "            and self.denoising_end < 1\n",
    "        ):\n",
    "            discrete_timestep_cutoff = int(\n",
    "                round(\n",
    "                    self.scheduler.config.num_train_timesteps\n",
    "                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)\n",
    "                )\n",
    "            )\n",
    "            num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n",
    "            timesteps = timesteps[:num_inference_steps]\n",
    "\n",
    "        # 9. Optionally get Guidance Scale Embedding\n",
    "        timestep_cond = None\n",
    "        if self.unet.config.time_cond_proj_dim is not None:\n",
    "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "            timestep_cond = self.get_guidance_scale_embedding(\n",
    "                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
    "            ).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "\n",
    "        ##################################\n",
    "        ########### MODIFIED #############\n",
    "        # Initialize\n",
    "        self.unet.GLOBAL_BUFFER.current_step = -1\n",
    "        ########### MODIFIED #############\n",
    "        ##################################\n",
    "        self._num_timesteps = len(timesteps)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
    "                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "                    added_cond_kwargs[\"image_embeds\"] = image_embeds\n",
    "\n",
    "                ##################################\n",
    "                ########### MODIFIED #############\n",
    "                # Before each U-Net forward pass, we increment the step\n",
    "                self.unet.GLOBAL_BUFFER.current_step += 1\n",
    "                ########### MODIFIED #############\n",
    "                ##################################\n",
    "\n",
    "\n",
    "                ##################################\n",
    "                ########### MODIFIED #############\n",
    "                \n",
    "                if self.unet.GLOBAL_BUFFER.score_mix:\n",
    "                    noise_pred = self.score_mix(latent_model_input,\n",
    "                                                t,\n",
    "                                                encoder_hidden_states=prompt_embeds,\n",
    "                                                timestep_cond=timestep_cond,\n",
    "                                                cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                                                added_cond_kwargs=added_cond_kwargs,\n",
    "                                                return_dict=False)\n",
    "                else:\n",
    "                    noise_pred = self.unet(\n",
    "                        latent_model_input,\n",
    "                        t,\n",
    "                        encoder_hidden_states=prompt_embeds,\n",
    "                        timestep_cond=timestep_cond,\n",
    "                        cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                        added_cond_kwargs=added_cond_kwargs,\n",
    "                        return_dict=False,\n",
    "                    )[0]\n",
    "                ########### MODIFIED #############\n",
    "                ##################################\n",
    "\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://huggingface.co/papers/2305.08891\n",
    "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents_dtype = latents.dtype\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "                if latents.dtype != latents_dtype:\n",
    "                    if torch.backends.mps.is_available():\n",
    "                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                        latents = latents.to(latents_dtype)\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    add_text_embeds = callback_outputs.pop(\"add_text_embeds\", add_text_embeds)\n",
    "                    add_time_ids = callback_outputs.pop(\"add_time_ids\", add_time_ids)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "                if XLA_AVAILABLE:\n",
    "                    xm.mark_step()\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # make sure the VAE is in float32 mode, as it overflows in float16\n",
    "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
    "\n",
    "            if needs_upcasting:\n",
    "                self.upcast_vae()\n",
    "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
    "            elif latents.dtype != self.vae.dtype:\n",
    "                if torch.backends.mps.is_available():\n",
    "                    # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                    self.vae = self.vae.to(latents.dtype)\n",
    "\n",
    "            # unscale/denormalize the latents\n",
    "            # denormalize with the mean and std if available and not None\n",
    "            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n",
    "            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n",
    "            if has_latents_mean and has_latents_std:\n",
    "                latents_mean = (\n",
    "                    torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "                )\n",
    "                latents_std = (\n",
    "                    torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "                )\n",
    "                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n",
    "            else:\n",
    "                latents = latents / self.vae.config.scaling_factor\n",
    "\n",
    "            image = self.vae.decode(latents, return_dict=False)[0]\n",
    "\n",
    "            # cast back to fp16 if needed\n",
    "            if needs_upcasting:\n",
    "                self.vae.to(dtype=torch.float16)\n",
    "        else:\n",
    "            image = latents\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # apply watermark if available\n",
    "            if self.watermark is not None:\n",
    "                image = self.watermark.apply_watermark(image)\n",
    "\n",
    "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusionXLPipelineOutput(images=image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4094d2",
   "metadata": {},
   "source": [
    "At this time, as can be seen in the implementation, when $\\lambda_t = 0$, the noise prediction from the FR-applied model is not needed, so we can confirm that it is implemented to skip it.\n",
    "\n",
    "```\n",
    "def score_mix(self, model_input, t, **kwargs):\n",
    "    total_step      = self.unet.GLOBAL_BUFFER.total_step\n",
    "    current_step    = self.unet.GLOBAL_BUFFER.current_step\n",
    "    skip_cnt        = self.unet.GLOBAL_BUFFER.skip_cnt\n",
    "\n",
    "    # Compute KeyFrame\n",
    "    if current_step in skip_cnt :\n",
    "        self.jump_score = self.unet(model_input, t, **kwargs)[0]\n",
    "        return self.jump_score\n",
    "    else :\n",
    "        step_ratio = current_step/total_step\n",
    "        bias       = self.unet.GLOBAL_BUFFER.bias\n",
    "        lamb       = self.lamb_schedule(step_ratio, _b=bias)\n",
    "\n",
    "        if DEBUG: print(lamb)\n",
    "        if lamb < 1e-10 :\n",
    "            # Reduced NFE\n",
    "            e_t = self.jump_score.   <------ 0에 가까울때는 계산하지 않고 스킵.\n",
    "        else :\n",
    "            # Score Mixing\n",
    "            noise_pred = self.unet(model_input, t, **kwargs)[0]\n",
    "\n",
    "            if type(noise_pred) == list or type(noise_pred) == tuple :\n",
    "                # Maybe this loop can be little slow..\n",
    "                e_t = [  self.mix_func(lamb, e_t_i, e_jump_i) for e_t_i , e_jump_i in zip(noise_pred, self.jump_score) ]\n",
    "            else :\n",
    "                e_t = self.mix_func(lamb, noise_pred, self.jump_score)\n",
    "        return e_t\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c4006",
   "metadata": {},
   "source": [
    "We apply this Score Mixing and generate the image again to measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b75167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapping_frdiff(pipe, total_step, skip_interval, bias=0.5, score_mix=True, debug=False):\n",
    "\n",
    "    specials = {\n",
    "    ResnetBlock2D : SkipResnetBlock2D,\n",
    "    Transformer2DModel : SkipTransformer2DModel,\n",
    "    }\n",
    "    global DEBUG\n",
    "    DEBUG=debug\n",
    "\n",
    "    # Set BUFFER\n",
    "    class BUFFER : pass\n",
    "    b = BUFFER()\n",
    "    pipe.GLOBAL_BUFFER = b\n",
    "\n",
    "    # Inject to every layer\n",
    "    for n, m in pipe.unet.named_modules() :\n",
    "        m.module_name = n\n",
    "        m.GLOBAL_BUFFER = b\n",
    "\n",
    "    pipe.unet.GLOBAL_BUFFER.total_step       = total_step\n",
    "    pipe.unet.GLOBAL_BUFFER.skip_interval    = skip_interval\n",
    "    pipe.unet.GLOBAL_BUFFER.bias             = bias\n",
    "    pipe.unet.GLOBAL_BUFFER.score_mix        = score_mix\n",
    "    pipe.unet.GLOBAL_BUFFER.skip_cnt         = list(range(0,total_step,skip_interval))\n",
    "    print(\"[SKIPPING] Buffer Sharing Complete\")\n",
    "\n",
    "    ### Patch Classes\n",
    "    # main\n",
    "    pipe.__class__ = SkipStableDiffusionXLPipeline\n",
    "\n",
    "    # Blocks\n",
    "    for n, m in pipe.unet.named_modules() :\n",
    "        if type(m) in specials.keys():\n",
    "            print(f'Convert {type(m)} to {type(m)}')\n",
    "            m.__class__ = specials[type(m)]\n",
    "\n",
    "            m.update()\n",
    "    print(\"[SKIPPING] Class Patching Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                                 variant=\"fp16\", torch_dtype=torch.float16,\n",
    "                                                 use_safetensors=True)\n",
    "\n",
    "pipe.to(f'cuda')\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "wrapping_frdiff(pipe, total_step=50, skip_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "t1 = time.time()\n",
    "image_fr_and_sm = pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]\n",
    "t2 = time.time()\n",
    "\n",
    "time_fr_and_sm = t2 - t1\n",
    "print(f\"Feature Reuse + Score Mixing Time : {time_fr_and_sm:.2f} seconds\")\n",
    "image_fr_and_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1883643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(image_ddim)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(f\"DDIM\\nTime: {time_ddim:.2f} seconds\")\n",
    "\n",
    "axes[1].imshow(image_fr)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(f\"FRDiff\\nTime: {time_fr:.2f} seconds\")\n",
    "\n",
    "axes[2].imshow(image_fr_and_sm)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title(f\"FR+SM\\nTime: {time_fr_and_sm:.2f} seconds\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e2211",
   "metadata": {},
   "source": [
    "In the case where SM is applied, we can see that the generation time becomes even faster,   \n",
    "while the low-frequency components—which preserve the morphological characteristics aligned with the original image—are maintained, resulting in better final generation quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
