{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0906f4",
   "metadata": {},
   "source": [
    "# FRDiff tutorial\n",
    "\n",
    "이 주피터 노트북은 \"FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models [ECCV24]\" 를 기반으로, Diffusion Caching기술에 대한 상세한 이해와 구현을 목표로 합니다.\n",
    "\n",
    "순서는 다음과 같습니다.\n",
    "- Diffusion Model의 Temporarl Redundancy 분석.\n",
    "- Feature Reuse의 구현\n",
    "- Score Mixing의 구현.\n",
    "- 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029d1e6",
   "metadata": {},
   "source": [
    "## Install Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do this on python 3.10\n",
    "You can create 3.10 kernel for ipynb by using this command on your shell\n",
    "\n",
    "conda create -n py310 python=3.10\n",
    "conda activate py310\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name py310 --display-name \"Python 3.10\"\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade diffusers==0.33.1\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac2735",
   "metadata": {},
   "source": [
    "## Standard Inference\n",
    "\n",
    "우선 아무 경량화 기술을 적용하지 않은 일반적인 Diffusion Model의 inference를 진행합니다.\n",
    "\n",
    "이 예시에서, 모델은 SDXL(Stable Diffusion XL)을 사용하고, Solver로는 DDIM 을 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c658b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "# Stable Diffusion 1.4\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", variant=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "# SDXL\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                                 variant=\"fp16\", torch_dtype=torch.float16,\n",
    "                                                 use_safetensors=True)\n",
    "\n",
    "pipe.to(f'cuda')\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30facc",
   "metadata": {},
   "source": [
    "50 step Denoising 으로 설정하고, prompt를 설정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c257f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "# Sampling Step\n",
    "num_steps = 50\n",
    "\n",
    "# Set your prompt !\n",
    "prompt = \"a photo of an astronaut riding a horse on mars, trending on artstation, high quality, detailed, cinematic lighting\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2e94c",
   "metadata": {},
   "source": [
    "### DDIM Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30216052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "t1 = time.time()\n",
    "image_ddim = pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]\n",
    "t2 = time.time()\n",
    "\n",
    "time_ddim = t2 - t1\n",
    "print(f\"DDIM Sampling Time: {time_ddim:.2f} seconds\")\n",
    "image_ddim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880352f",
   "metadata": {},
   "source": [
    "이미지가 정상적으로 생성되는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a695bf4",
   "metadata": {},
   "source": [
    "### Temporal Redundancy Analysis\n",
    "\n",
    "SDXL은 Unet모델을 사용하며, 이는 주로 두가지 종류의 블록으로 구성되어 있습니다.\n",
    "\n",
    "- ResnetBlock2D\n",
    "- Transformer2DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7728152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from typing import Union, Optional, Dict, Callable, List, Any, Tuple\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    StableDiffusionXLPipelineOutput,\n",
    "    retrieve_timesteps,\n",
    "    rescale_noise_cfg,\n",
    "    XLA_AVAILABLE,\n",
    ")\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import (\n",
    "    StableDiffusionPipeline, StableDiffusionPipelineOutput\n",
    ")\n",
    "\n",
    "from diffusers.models.resnet import ResnetBlock2D\n",
    "from diffusers.models.upsampling import Upsample2D\n",
    "from diffusers.models.downsampling import Downsample2D\n",
    "from diffusers.models.transformers.transformer_2d import Transformer2DModel, Transformer2DModelOutput\n",
    "from diffusers.utils import is_torch_version, USE_PEFT_BACKEND, deprecate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n , m in pipe.unet.named_modules():\n",
    "    if isinstance(m, ResnetBlock2D):\n",
    "        print(m.__class__.__name__, n)\n",
    "    elif isinstance(m, Transformer2DModel):\n",
    "        print(m.__class__.__name__, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d77c69",
   "metadata": {},
   "source": [
    "위에서 볼 수 있듯, ResBlock과 TransformerBlock 이 번갈아가며 나타나는 구조임을 알 수 있습니다.\n",
    "\n",
    "Temporal Redundancy를 분석하기 위해, 우선은 ResBlock에 대해서만 분석하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9477112",
   "metadata": {},
   "source": [
    "### ResnetBlock2D\n",
    "\n",
    "아래는 SDXL의 ResBlock의 forward path에 대한 *원본 구현*입니다. \n",
    "\n",
    "원본 코드 'https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py' 에서도 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0521115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "    if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "        deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "        deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "    hidden_states = input_tensor\n",
    "\n",
    "    hidden_states = self.norm1(hidden_states)\n",
    "    hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "    if self.upsample is not None:\n",
    "        # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "        if hidden_states.shape[0] >= 64:\n",
    "            input_tensor = input_tensor.contiguous()\n",
    "            hidden_states = hidden_states.contiguous()\n",
    "        input_tensor = self.upsample(input_tensor)\n",
    "        hidden_states = self.upsample(hidden_states)\n",
    "    elif self.downsample is not None:\n",
    "        input_tensor = self.downsample(input_tensor)\n",
    "        hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "    hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "    if self.time_emb_proj is not None:\n",
    "        if not self.skip_time_act:\n",
    "            temb = self.nonlinearity(temb)\n",
    "        temb = self.time_emb_proj(temb)[:, :, None, None]\n",
    "\n",
    "    if self.time_embedding_norm == \"default\":\n",
    "        if temb is not None:\n",
    "            hidden_states = hidden_states + temb\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "    elif self.time_embedding_norm == \"scale_shift\":\n",
    "        if temb is None:\n",
    "            raise ValueError(\n",
    "                f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n",
    "            )\n",
    "        time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "        hidden_states = hidden_states * (1 + time_scale) + time_shift\n",
    "    else:\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "    hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "    hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "    if self.conv_shortcut is not None:\n",
    "        input_tensor = self.conv_shortcut(input_tensor.contiguous())\n",
    "\n",
    "    output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9200177",
   "metadata": {},
   "source": [
    "코드에서 확인할 수 있듯, 다음과 같은 연산 구조로 이루어져 있음을 알 수 있습니다.\n",
    "\n",
    "1. res <- x\n",
    "2. x <- Conv1(x)\n",
    "3. x <- Conv2 ( NonLinear( x+ t ) )\n",
    "4. x <- res + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b32799",
   "metadata": {},
   "source": [
    "Temporal Redundancy 분석을 위한 Logging code를 삽입해보도록 하겠습니다.\n",
    "\n",
    "\\#\\#\\#\\# MODIFED \\#\\#\\#\\# 로 감싸진 부분이 수정된 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe6427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieved from Original repo : 'https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py\n",
    "class SkipResnetBlock2D(ResnetBlock2D):\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ############## MODIFIED ###############\n",
    "        self.res_memory.append(hidden_states.cpu())\n",
    "        ############## MODIFIED ###############\n",
    "        #######################################\n",
    "\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        if self.time_emb_proj is not None:\n",
    "            if not self.skip_time_act:\n",
    "                temb = self.nonlinearity(temb)\n",
    "            temb = self.time_emb_proj(temb)[:, :, None, None]\n",
    "\n",
    "        if self.time_embedding_norm == \"default\":\n",
    "            if temb is not None:\n",
    "                hidden_states = hidden_states + temb\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "        elif self.time_embedding_norm == \"scale_shift\":\n",
    "            if temb is None:\n",
    "                raise ValueError(\n",
    "                    f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n",
    "                )\n",
    "            time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "            hidden_states = hidden_states * (1 + time_scale) + time_shift\n",
    "        else:\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            input_tensor = self.conv_shortcut(input_tensor.contiguous())\n",
    "\n",
    "        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d54dfd",
   "metadata": {},
   "source": [
    "Logging code를 삽입할 레이어를 선택하고, `ResnetBlock2D` 클래스를, 상속한 `SkipResnetBlock2D`로 클래스를 바꿔 logging code를 injection합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = ['down_blocks.0.resnets.0','mid_block.resnets.0']\n",
    "\n",
    "# Patch Classes\n",
    "for n, m in pipe.unet.named_modules() :\n",
    "    #print(n)\n",
    "    if n in target_layers:\n",
    "        #print(n)\n",
    "        print(f'Convert {type(m)} to SkipResnetBlock2D')\n",
    "        m.__class__ = SkipResnetBlock2D\n",
    "        m.res_memory = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534ecc6",
   "metadata": {},
   "source": [
    "이미지를 생성하여 로그를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940121cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49667f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "\n",
    "for n,m in pipe.unet.named_modules():\n",
    "    if isinstance(m, SkipResnetBlock2D):\n",
    "        print(f'Collecting {n} logs')\n",
    "        logs.append(m.res_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b8b93",
   "metadata": {},
   "source": [
    "이제 가져온 로그로, Temporal Redundancy를 분석합니다.\n",
    "\n",
    "우선, 각 타임스텝마다 저장한 feature tensor를 한 채널에 대해 Visualize함으로써, 시간에 따른 feature의 변화를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log = logs[0]\n",
    "channel_num = 1\n",
    "\n",
    "num_features = len(log)\n",
    "cols = 10\n",
    "rows = (num_features + cols - 1) // cols  # 올림\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "\n",
    "for idx, t_feature in enumerate(log):\n",
    "    r = idx // cols\n",
    "    c = idx % cols\n",
    "    t_img = t_feature[0][channel_num].cpu().numpy()\n",
    "    axes[r, c].imshow(t_img, cmap='gray')\n",
    "    axes[r, c].axis('off')\n",
    "    axes[r, c].set_title(f\"t={idx}\")\n",
    "\n",
    "# 남는 subplot은 안보이게\n",
    "for idx in range(num_features, rows * cols):\n",
    "    r = idx // cols\n",
    "    c = idx % cols\n",
    "    axes[r, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdcf3c",
   "metadata": {},
   "source": [
    "위에서 확인할 수 있듯, 이러한 featuer들은 연속된 시간변화에 대해 매우 작은 변화량만을 가집니다.\n",
    "\n",
    "아래와 같이 이를 수치적으로도 분석할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for layer_log in logs :\n",
    "    errs = []\n",
    "    for t in range(1, len(layer_log)):\n",
    "        err = (layer_log[t] - layer_log[t-1]).abs().mean().item()\n",
    "        errs.append(err)\n",
    "\n",
    "    \n",
    "    plt.plot(errs)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.title(\"Temporal Feature Differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2358db6",
   "metadata": {},
   "source": [
    "마찬가지로 작은 수치적 에러를 가짐을 확인할 수 있습니다. 이러한 작은 변화량은, 한번 계산한 피쳐를 다음 스텝에 재사용함으로써 계산량을 크게 줄일 수 있는 가능성을 시사합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c3249",
   "metadata": {},
   "source": [
    "## FRDiff\n",
    "\n",
    "이제 저희 연구인 FRDiff의 구현을 다뤄볼 것입니다. FRDiff는 총 세가지 컴포넌트인\n",
    "- Feature Reuse\n",
    "- Score Mixing\n",
    "- Auto-FR\n",
    "\n",
    "로 진행되지만, 이 튜토리얼에서는 Feature-Reuse와 Score-Mixing에 대해서만 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc9f3f",
   "metadata": {},
   "source": [
    "### 1. Feature-Reuse\n",
    "\n",
    "Feature Reuse는 다음과 같은 formulation을 가집니다.\n",
    "\n",
    "우선, `ResBlock`이나 `TransformerBlock` 같은 Residual 구조를 가지는 모든 블록 $ y \\leftarrow x + \\mathcal{F}(x,t)$ 들의 non residual $\\mathcal{F}(x,t)$ 부분을, 다음과 같은 형태로 decompose합니다.\n",
    "\n",
    "$\\mathcal{F}(x_t,t) \\leftarrow f( \\mathcal{S}(x_t), t)$\n",
    "\n",
    "이때 $\\mathcal{S}(x_t)$ 는 time step 에 대한 정보를 계산하지 않는 부분으로, *Spatial Block*이라고 지칭합니다.\n",
    "\n",
    "예를 들어, 이전 ResBLock구조였던\n",
    "\n",
    "1. res <- x\n",
    "2. x <- Conv1(x)\n",
    "3. x <- Conv2 ( NonLinear( x+ t ) )\n",
    "4. x <- res + x\n",
    "\n",
    "에서는, 2. x <- Conv1(x) 부분이 *Spatial Block*에 해당합니다. \n",
    "\n",
    "---\n",
    "\n",
    "다음으로, Feature Caching을 수행할 지점인 *Keyframe set*  $\\mathcal{K}$ (i.e, {10,20,30,40,50} )을 정의하고, \n",
    "\n",
    "현재 denosing step $t$가, $t \\in \\mathcal{K}$ 일때, Caching 을 진행합니다.  \n",
    "$if\\ \\  t \\in \\mathcal{K} :$  \n",
    "$\\quad M \\leftarrow S(x_t)$  \n",
    "$y \\leftarrow f(M, t) + x_t$\n",
    "\n",
    "---\n",
    "\n",
    "아래는 이에 따른 ResBlock에서의 구현입니다. \\#\\#\\#\\# MODIFED \\#\\#\\#\\# 로 감싸진 부분이 수정된 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c978cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieved from Original repo : 'https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py\n",
    "class SkipResnetBlock2D(ResnetBlock2D):\n",
    "    \n",
    "    ### MODIFIED #####\n",
    "    # cleaner\n",
    "    def update(self):\n",
    "        self.layer_memory = None\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ############## MODIFIED ###############\n",
    "        # Original Forward\n",
    "        # hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        # Our Feature Reuse\n",
    "        if self.GLOBAL_BUFFER.current_step in self.GLOBAL_BUFFER.skip_cnt: # if t \\in K\n",
    "            self.layer_memory = self.conv1(hidden_states)\n",
    "        hidden_states = self.layer_memory\n",
    "        ############## MODIFIED ###############\n",
    "        #######################################\n",
    "\n",
    "\n",
    "        if self.time_emb_proj is not None:\n",
    "            if not self.skip_time_act:\n",
    "                temb = self.nonlinearity(temb)\n",
    "            temb = self.time_emb_proj(temb)[:, :, None, None]\n",
    "\n",
    "        if self.time_embedding_norm == \"default\":\n",
    "            if temb is not None:\n",
    "                hidden_states = hidden_states + temb\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "        elif self.time_embedding_norm == \"scale_shift\":\n",
    "            if temb is None:\n",
    "                raise ValueError(\n",
    "                    f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n",
    "                )\n",
    "            time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "            hidden_states = hidden_states * (1 + time_scale) + time_shift\n",
    "        else:\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            input_tensor = self.conv_shortcut(input_tensor.contiguous())\n",
    "\n",
    "        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901fd98",
   "metadata": {},
   "source": [
    "다음은 Transformer2D Block에서의 구현입니다.\n",
    "Transformerblock은 블록 전체가 인풋으로 timestep정보를 사용하지 않기 때문에, 단순히 residual 파트만 분리하여 reuse할 수 있습니다. \n",
    "\n",
    "따라서 다음과 같이 구조로 구현됩니다. (_forward는 기존 forward function.)\n",
    "\n",
    "```\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states, ...):\n",
    "\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Feature Reuse\n",
    "        if t in KEYFRAME_SET:\n",
    "            self.layer_memory = self._forward(hidden_states,  ... )[0] - residual\n",
    "            \n",
    "        output = self.layer_memory + residual\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7280291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipTransformer2DModel(Transformer2DModel):\n",
    "    def update(self):\n",
    "        self.layer_memory = None\n",
    "\n",
    "    #### MODIFIED #####\n",
    "    #override forward function.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        timestep: Optional[torch.LongTensor] = None,\n",
    "        added_cond_kwargs: Dict[str, torch.Tensor] = None,\n",
    "        class_labels: Optional[torch.LongTensor] = None,\n",
    "        cross_attention_kwargs: Dict[str, Any] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ):\n",
    "        assert self.is_input_continuous is True\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Feature Reuse\n",
    "        if self.GLOBAL_BUFFER.current_step in self.GLOBAL_BUFFER.skip_cnt:\n",
    "\n",
    "            self.layer_memory = self._forward(hidden_states, encoder_hidden_states,\n",
    "                                            timestep, added_cond_kwargs, class_labels,\n",
    "                                            cross_attention_kwargs, attention_mask,\n",
    "                                            encoder_attention_mask, return_dict)[0] - residual\n",
    "            # We only cache the non-residual output part. So we subtract the residual from the output.\n",
    "\n",
    "\n",
    "        output = self.layer_memory + residual\n",
    "\n",
    "        if not return_dict:\n",
    "            return (output,)\n",
    "\n",
    "        return Transformer2DModelOutput(sample=output)\n",
    "\n",
    "    #### Original Forward ####\n",
    "    def _forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        timestep: Optional[torch.LongTensor] = None,\n",
    "        added_cond_kwargs: Dict[str, torch.Tensor] = None,\n",
    "        class_labels: Optional[torch.LongTensor] = None,\n",
    "        cross_attention_kwargs: Dict[str, Any] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The [`Transformer2DModel`] forward method.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (`torch.LongTensor` of shape `(batch size, num latent pixels)` if discrete, `torch.Tensor` of shape `(batch size, channel, height, width)` if continuous):\n",
    "                Input `hidden_states`.\n",
    "            encoder_hidden_states ( `torch.Tensor` of shape `(batch size, sequence len, embed dims)`, *optional*):\n",
    "                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to\n",
    "                self-attention.\n",
    "            timestep ( `torch.LongTensor`, *optional*):\n",
    "                Used to indicate denoising step. Optional timestep to be applied as an embedding in `AdaLayerNorm`.\n",
    "            class_labels ( `torch.LongTensor` of shape `(batch size, num classes)`, *optional*):\n",
    "                Used to indicate class labels conditioning. Optional class labels to be applied as an embedding in\n",
    "                `AdaLayerZeroNorm`.\n",
    "            cross_attention_kwargs ( `Dict[str, Any]`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n",
    "            attention_mask ( `torch.Tensor`, *optional*):\n",
    "                An attention mask of shape `(batch, key_tokens)` is applied to `encoder_hidden_states`. If `1` the mask\n",
    "                is kept, otherwise if `0` it is discarded. Mask will be converted into a bias, which adds large\n",
    "                negative values to the attention scores corresponding to \"discard\" tokens.\n",
    "            encoder_attention_mask ( `torch.Tensor`, *optional*):\n",
    "                Cross-attention mask applied to `encoder_hidden_states`. Two formats supported:\n",
    "\n",
    "                    * Mask `(batch, sequence_length)` True = keep, False = discard.\n",
    "                    * Bias `(batch, 1, sequence_length)` 0 = keep, -10000 = discard.\n",
    "\n",
    "                If `ndim == 2`: will be interpreted as a mask, then converted into a bias consistent with the format\n",
    "                above. This bias will be added to the cross-attention scores.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.unets.unet_2d_condition.UNet2DConditionOutput`] instead of a plain\n",
    "                tuple.\n",
    "\n",
    "        Returns:\n",
    "            If `return_dict` is True, an [`~models.transformers.transformer_2d.Transformer2DModelOutput`] is returned,\n",
    "            otherwise a `tuple` where the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "        if cross_attention_kwargs is not None:\n",
    "            if cross_attention_kwargs.get(\"scale\", None) is not None:\n",
    "                logger.warning(\"Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.\")\n",
    "        # ensure attention_mask is a bias, and give it a singleton query_tokens dimension.\n",
    "        #   we may have done this conversion already, e.g. if we came here via UNet2DConditionModel#forward.\n",
    "        #   we can tell by counting dims; if ndim == 2: it's a mask rather than a bias.\n",
    "        # expects mask of shape:\n",
    "        #   [batch, key_tokens]\n",
    "        # adds singleton query_tokens dimension:\n",
    "        #   [batch,                    1, key_tokens]\n",
    "        # this helps to broadcast it as a bias over attention scores, which will be in one of the following shapes:\n",
    "        #   [batch,  heads, query_tokens, key_tokens] (e.g. torch sdp attn)\n",
    "        #   [batch * heads, query_tokens, key_tokens] (e.g. xformers or classic attn)\n",
    "        if attention_mask is not None and attention_mask.ndim == 2:\n",
    "            # assume that mask is expressed as:\n",
    "            #   (1 = keep,      0 = discard)\n",
    "            # convert mask into a bias that can be added to attention scores:\n",
    "            #       (keep = +0,     discard = -10000.0)\n",
    "            attention_mask = (1 - attention_mask.to(hidden_states.dtype)) * -10000.0\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "        # convert encoder_attention_mask to a bias the same way we do for attention_mask\n",
    "        if encoder_attention_mask is not None and encoder_attention_mask.ndim == 2:\n",
    "            encoder_attention_mask = (1 - encoder_attention_mask.to(hidden_states.dtype)) * -10000.0\n",
    "            encoder_attention_mask = encoder_attention_mask.unsqueeze(1)\n",
    "\n",
    "        # 1. Input\n",
    "        if self.is_input_continuous:\n",
    "            batch_size, _, height, width = hidden_states.shape\n",
    "            residual = hidden_states\n",
    "            hidden_states, inner_dim = self._operate_on_continuous_inputs(hidden_states)\n",
    "        elif self.is_input_vectorized:\n",
    "            hidden_states = self.latent_image_embedding(hidden_states)\n",
    "        elif self.is_input_patches:\n",
    "            height, width = hidden_states.shape[-2] // self.patch_size, hidden_states.shape[-1] // self.patch_size\n",
    "            hidden_states, encoder_hidden_states, timestep, embedded_timestep = self._operate_on_patched_inputs(\n",
    "                hidden_states, encoder_hidden_states, timestep, added_cond_kwargs\n",
    "            )\n",
    "\n",
    "        # 2. Blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            if torch.is_grad_enabled() and self.gradient_checkpointing:\n",
    "                hidden_states = self._gradient_checkpointing_func(\n",
    "                    block,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    timestep,\n",
    "                    cross_attention_kwargs,\n",
    "                    class_labels,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = block(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    timestep=timestep,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    class_labels=class_labels,\n",
    "                )\n",
    "\n",
    "\n",
    "    \n",
    "        # 3. Output\n",
    "        if self.is_input_continuous:\n",
    "            output = self._get_output_for_continuous_inputs(\n",
    "                hidden_states=hidden_states,\n",
    "                residual=residual,\n",
    "                batch_size=batch_size,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                inner_dim=inner_dim,\n",
    "            )\n",
    "        elif self.is_input_vectorized:\n",
    "            output = self._get_output_for_vectorized_inputs(hidden_states)\n",
    "        elif self.is_input_patches:\n",
    "            output = self._get_output_for_patched_inputs(\n",
    "                hidden_states=hidden_states,\n",
    "                timestep=timestep,\n",
    "                class_labels=class_labels,\n",
    "                embedded_timestep=embedded_timestep,\n",
    "                height=height,\n",
    "                width=width,\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            return (output,)\n",
    "\n",
    "        return Transformer2DModelOutput(sample=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9057011",
   "metadata": {},
   "source": [
    "마지막으로, `GLOBAL_BUFFER.current_step`에 현재 Denoising timestep을 기록하기 위해 Pipeline function도 아래와 같이 수정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97776cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipStableDiffusionXLPipeline(StableDiffusionXLPipeline):\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        denoising_end: Optional[float] = None,\n",
    "        guidance_scale: float = 5.0,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        ip_adapter_image : Any = None,  # PipelineImageInput\n",
    "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        original_size: Optional[Tuple[int, int]] = None,\n",
    "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        target_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end : Any = None,  # Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        callback = kwargs.pop(\"callback\", None)\n",
    "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "        if callback is not None:\n",
    "            deprecate(\n",
    "                \"callback\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "        if callback_steps is not None:\n",
    "            deprecate(\n",
    "                \"callback_steps\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "\n",
    "        #if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "        #    callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.default_sample_size * self.vae_scale_factor\n",
    "        width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "        original_size = original_size or (height, width)\n",
    "        target_size = target_size or (height, width)\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            prompt_2,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            negative_prompt_2,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            callback_on_step_end_tensor_inputs,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._guidance_rescale = guidance_rescale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._cross_attention_kwargs = cross_attention_kwargs\n",
    "        self._denoising_end = denoising_end\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        lora_scale = (\n",
    "            self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
    "        )\n",
    "\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt=prompt,\n",
    "            prompt_2=prompt_2,\n",
    "            device=device,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_2=negative_prompt_2,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            lora_scale=lora_scale,\n",
    "            clip_skip=self.clip_skip,\n",
    "        )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(\n",
    "            self.scheduler, num_inference_steps, device, timesteps, sigmas\n",
    "        )\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Prepare added time ids & embeddings\n",
    "        add_text_embeds = pooled_prompt_embeds\n",
    "        if self.text_encoder_2 is None:\n",
    "            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
    "        else:\n",
    "            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n",
    "\n",
    "        add_time_ids = self._get_add_time_ids(\n",
    "            original_size,\n",
    "            crops_coords_top_left,\n",
    "            target_size,\n",
    "            dtype=prompt_embeds.dtype,\n",
    "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "        )\n",
    "        if negative_original_size is not None and negative_target_size is not None:\n",
    "            negative_add_time_ids = self._get_add_time_ids(\n",
    "                negative_original_size,\n",
    "                negative_crops_coords_top_left,\n",
    "                negative_target_size,\n",
    "                dtype=prompt_embeds.dtype,\n",
    "                text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "            )\n",
    "        else:\n",
    "            negative_add_time_ids = add_time_ids\n",
    "\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
    "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
    "\n",
    "        prompt_embeds = prompt_embeds.to(device)\n",
    "        add_text_embeds = add_text_embeds.to(device)\n",
    "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
    "\n",
    "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "\n",
    "        # 8.1 Apply denoising_end\n",
    "        if (\n",
    "            self.denoising_end is not None\n",
    "            and isinstance(self.denoising_end, float)\n",
    "            and self.denoising_end > 0\n",
    "            and self.denoising_end < 1\n",
    "        ):\n",
    "            discrete_timestep_cutoff = int(\n",
    "                round(\n",
    "                    self.scheduler.config.num_train_timesteps\n",
    "                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)\n",
    "                )\n",
    "            )\n",
    "            num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n",
    "            timesteps = timesteps[:num_inference_steps]\n",
    "\n",
    "        # 9. Optionally get Guidance Scale Embedding\n",
    "        timestep_cond = None\n",
    "        if self.unet.config.time_cond_proj_dim is not None:\n",
    "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "            timestep_cond = self.get_guidance_scale_embedding(\n",
    "                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
    "            ).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "\n",
    "        ##################################\n",
    "        ########### MODIFIED #############\n",
    "        # Initialize\n",
    "        self.unet.GLOBAL_BUFFER.current_step = -1\n",
    "        ########### MODIFIED #############\n",
    "        ##################################\n",
    "        self._num_timesteps = len(timesteps)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
    "                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "                    added_cond_kwargs[\"image_embeds\"] = image_embeds\n",
    "\n",
    "                ##################################\n",
    "                ########### MODIFIED #############\n",
    "                # Before each U-Net forward pass, we increment the step\n",
    "                self.unet.GLOBAL_BUFFER.current_step += 1\n",
    "                ########### MODIFIED #############\n",
    "                ##################################\n",
    "                noise_pred = self.unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    timestep_cond=timestep_cond,\n",
    "                    cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                    added_cond_kwargs=added_cond_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://huggingface.co/papers/2305.08891\n",
    "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents_dtype = latents.dtype\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "                if latents.dtype != latents_dtype:\n",
    "                    if torch.backends.mps.is_available():\n",
    "                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                        latents = latents.to(latents_dtype)\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    add_text_embeds = callback_outputs.pop(\"add_text_embeds\", add_text_embeds)\n",
    "                    add_time_ids = callback_outputs.pop(\"add_time_ids\", add_time_ids)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "                if XLA_AVAILABLE:\n",
    "                    xm.mark_step()\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # make sure the VAE is in float32 mode, as it overflows in float16\n",
    "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
    "\n",
    "            if needs_upcasting:\n",
    "                self.upcast_vae()\n",
    "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
    "            elif latents.dtype != self.vae.dtype:\n",
    "                if torch.backends.mps.is_available():\n",
    "                    # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                    self.vae = self.vae.to(latents.dtype)\n",
    "\n",
    "            # unscale/denormalize the latents\n",
    "            # denormalize with the mean and std if available and not None\n",
    "            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n",
    "            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n",
    "            if has_latents_mean and has_latents_std:\n",
    "                latents_mean = (\n",
    "                    torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "                )\n",
    "                latents_std = (\n",
    "                    torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "                )\n",
    "                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n",
    "            else:\n",
    "                latents = latents / self.vae.config.scaling_factor\n",
    "\n",
    "            image = self.vae.decode(latents, return_dict=False)[0]\n",
    "\n",
    "            # cast back to fp16 if needed\n",
    "            if needs_upcasting:\n",
    "                self.vae.to(dtype=torch.float16)\n",
    "        else:\n",
    "            image = latents\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # apply watermark if available\n",
    "            if self.watermark is not None:\n",
    "                image = self.watermark.apply_watermark(image)\n",
    "\n",
    "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusionXLPipelineOutput(images=image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567eb261",
   "metadata": {},
   "source": [
    "마지막으로, 모든 디퓨전 모델의 모든 클래스를 새롭게 수정한 클래스로 injection 해줍니다. \n",
    "\n",
    "이때, 모든 레이어 인스턴스가 참조할 수 있는 global variable buffer를 만들어 주기 위해 빈 클래스의 인스턴스를 만들어 넘겨줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aa6a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapping_frdiff(pipe, total_step, skip_interval, bias=0.5, score_mix=True, debug=False):\n",
    "\n",
    "    specials = {\n",
    "    ResnetBlock2D : SkipResnetBlock2D,\n",
    "    Transformer2DModel : SkipTransformer2DModel,\n",
    "    }\n",
    "    global DEBUG\n",
    "    DEBUG=debug\n",
    "\n",
    "    # Set BUFFER\n",
    "    class BUFFER : pass\n",
    "    b = BUFFER()\n",
    "    pipe.GLOBAL_BUFFER = b\n",
    "\n",
    "    # Inject to every layer\n",
    "    for n, m in pipe.unet.named_modules() :\n",
    "        m.module_name = n\n",
    "        m.GLOBAL_BUFFER = b\n",
    "\n",
    "    pipe.unet.GLOBAL_BUFFER.total_step       = total_step\n",
    "    pipe.unet.GLOBAL_BUFFER.skip_interval    = skip_interval\n",
    "    pipe.unet.GLOBAL_BUFFER.bias             = bias\n",
    "    pipe.unet.GLOBAL_BUFFER.score_mix        = score_mix\n",
    "    pipe.unet.GLOBAL_BUFFER.skip_cnt         = list(range(0,total_step,skip_interval))\n",
    "    print(\"[SKIPPING] Buffer Sharing Complete\")\n",
    "\n",
    "    ### Patch Classes\n",
    "    # main\n",
    "    pipe.__class__ = SkipStableDiffusionXLPipeline\n",
    "\n",
    "    # Blocks\n",
    "    for n, m in pipe.unet.named_modules() :\n",
    "        if type(m) in specials.keys():\n",
    "            print(f'Convert {type(m)} to {type(m)}')\n",
    "            m.__class__ = specials[type(m)]\n",
    "\n",
    "            m.update()\n",
    "    print(\"[SKIPPING] Class Patching Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334aebb5",
   "metadata": {},
   "source": [
    "pipeline을 새로 만들고 wrapper를 적용합니다. 우선 예시로 interval=5 의 경우, 즉 keyframe set이 {0,5,10,15, ...} 인 경우로 생성합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c87f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                                 variant=\"fp16\", torch_dtype=torch.float16,\n",
    "                                                 use_safetensors=True)\n",
    "\n",
    "pipe.to(f'cuda')\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "wrapping_frdiff(pipe, total_step=50, skip_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c40c7",
   "metadata": {},
   "source": [
    "Feature Reuse를 적용한 pipeline으로 이미지를 새로 생성하고, 시간을 측정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14107c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "t1 = time.time()\n",
    "image_fr = pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]\n",
    "t2 = time.time()\n",
    "\n",
    "time_fr = t2 - t1\n",
    "print(f\"Feature Reuse Time : {time_fr:.2f} seconds\")\n",
    "image_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5806cf5",
   "metadata": {},
   "source": [
    "두 이미지의 품질과 시간을 비교해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2302a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(image_ddim)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(f\"DDIM\\nTime: {time_ddim:.2f} seconds\")\n",
    "\n",
    "axes[1].imshow(image_fr)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(f\"FRDiff\\nTime: {time_fr:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd61315",
   "metadata": {},
   "source": [
    "생성품질이 약간 저하되었지만 훨씬 더 빠른 시간에 생성 된것을 알 수 있습니다. \n",
    "\n",
    "keyframe interval을 조절해, 생성품질과 생성시간 사이의 trade-off를 조절해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7def4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL = 2\n",
    "\n",
    "pipe2 = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                                 variant=\"fp16\", torch_dtype=torch.float16,\n",
    "                                                 use_safetensors=True)\n",
    "\n",
    "pipe2.to(f'cuda')\n",
    "pipe2.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "wrapping_frdiff(pipe2, total_step=50, skip_interval=INTERVAL)\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae506c",
   "metadata": {},
   "source": [
    "### 2. Score Mixing\n",
    "\n",
    "다음은 Score Mixing으로, 최종 이미지의 low-frequency정보를 주로 담고 있는 early denoising 과정에서의 noise prediction을 이용해, FR로 인해 저하된 생성품질을 높이기 위한 기술입니다.\n",
    "\n",
    "Diffusion model의 원본 denoising 과정은 아래와 같습니다. \n",
    "\n",
    "$ x_{t+1} \\leftarrow \\epsilon_\\theta(x_t, t)$ \n",
    "\n",
    "이때 $\\epsilon_\\theta$는 neural network, SDXL에서는 u-net을 의미합니다.\n",
    "\n",
    "Score Mixing을 적용한 Denoising 과정은 아래와 같이 표현됩니다.\n",
    "\n",
    "$ if\\quad t \\in \\mathcal{K} :$  \n",
    "$ \\quad E \\leftarrow \\epsilon_\\theta(x_t, t)$   \n",
    "$ x_{t+1} \\leftarrow \\lambda_t \\cdot \\epsilon_\\theta(x_t, t) + ( 1 - \\lambda_t) \\cdot E$ \n",
    "\n",
    "\n",
    "output 단위에서 적용되므로, 위의 StableDiffusionXLPipeline을 다음과 같이 수정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adb7ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipStableDiffusionXLPipeline(StableDiffusionXLPipeline):\n",
    "\n",
    "    def lamb_schedule(self, step_ratio, _t=30, _b=0.5) :\n",
    "        lamb = np.maximum(0.0, np.minimum(1, (_t * (step_ratio - _b) + 2)/4))\n",
    "        lamb = float(lamb)\n",
    "        #lamb = 1 #Skip\n",
    "        #lamb = 0 #Jump\n",
    "        return lamb\n",
    "\n",
    "    def mix_func(self,lamb, e_t, e_jump) :\n",
    "        return lamb*e_t + (1-lamb)*e_jump\n",
    "\n",
    "    def score_mix(self, model_input, t, **kwargs):\n",
    "        total_step      = self.unet.GLOBAL_BUFFER.total_step\n",
    "        current_step    = self.unet.GLOBAL_BUFFER.current_step\n",
    "        skip_cnt        = self.unet.GLOBAL_BUFFER.skip_cnt\n",
    "\n",
    "        # Compute KeyFrame\n",
    "        if current_step in skip_cnt :\n",
    "            self.jump_score = self.unet(model_input, t, **kwargs)[0]\n",
    "            return self.jump_score\n",
    "        else :\n",
    "            step_ratio = current_step/total_step\n",
    "            bias       = self.unet.GLOBAL_BUFFER.bias\n",
    "            lamb       = self.lamb_schedule(step_ratio, _b=bias)\n",
    "\n",
    "            if DEBUG: print(lamb)\n",
    "            if lamb < 1e-10 :\n",
    "                # Reduced NFE\n",
    "                e_t = self.jump_score\n",
    "            else :\n",
    "                # Score Mixing\n",
    "                noise_pred = self.unet(model_input, t, **kwargs)[0]\n",
    "\n",
    "                if type(noise_pred) == list or type(noise_pred) == tuple :\n",
    "                    # Maybe this loop can be little slow..\n",
    "                    e_t = [  self.mix_func(lamb, e_t_i, e_jump_i) for e_t_i , e_jump_i in zip(noise_pred, self.jump_score) ]\n",
    "                else :\n",
    "                    e_t = self.mix_func(lamb, noise_pred, self.jump_score)\n",
    "            return e_t\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        timesteps: List[int] = None,\n",
    "        sigmas: List[float] = None,\n",
    "        denoising_end: Optional[float] = None,\n",
    "        guidance_scale: float = 5.0,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        ip_adapter_image : Any = None,  # PipelineImageInput\n",
    "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        original_size: Optional[Tuple[int, int]] = None,\n",
    "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        target_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
    "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end : Any = None,  # Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        callback = kwargs.pop(\"callback\", None)\n",
    "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "        if callback is not None:\n",
    "            deprecate(\n",
    "                \"callback\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "        if callback_steps is not None:\n",
    "            deprecate(\n",
    "                \"callback_steps\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "\n",
    "        #if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "        #    callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.default_sample_size * self.vae_scale_factor\n",
    "        width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "        original_size = original_size or (height, width)\n",
    "        target_size = target_size or (height, width)\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            prompt_2,\n",
    "            height,\n",
    "            width,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            negative_prompt_2,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            callback_on_step_end_tensor_inputs,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._guidance_rescale = guidance_rescale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._cross_attention_kwargs = cross_attention_kwargs\n",
    "        self._denoising_end = denoising_end\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        lora_scale = (\n",
    "            self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
    "        )\n",
    "\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt=prompt,\n",
    "            prompt_2=prompt_2,\n",
    "            device=device,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_2=negative_prompt_2,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            lora_scale=lora_scale,\n",
    "            clip_skip=self.clip_skip,\n",
    "        )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(\n",
    "            self.scheduler, num_inference_steps, device, timesteps, sigmas\n",
    "        )\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Prepare added time ids & embeddings\n",
    "        add_text_embeds = pooled_prompt_embeds\n",
    "        if self.text_encoder_2 is None:\n",
    "            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
    "        else:\n",
    "            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n",
    "\n",
    "        add_time_ids = self._get_add_time_ids(\n",
    "            original_size,\n",
    "            crops_coords_top_left,\n",
    "            target_size,\n",
    "            dtype=prompt_embeds.dtype,\n",
    "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "        )\n",
    "        if negative_original_size is not None and negative_target_size is not None:\n",
    "            negative_add_time_ids = self._get_add_time_ids(\n",
    "                negative_original_size,\n",
    "                negative_crops_coords_top_left,\n",
    "                negative_target_size,\n",
    "                dtype=prompt_embeds.dtype,\n",
    "                text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "            )\n",
    "        else:\n",
    "            negative_add_time_ids = add_time_ids\n",
    "\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
    "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
    "\n",
    "        prompt_embeds = prompt_embeds.to(device)\n",
    "        add_text_embeds = add_text_embeds.to(device)\n",
    "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
    "\n",
    "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "\n",
    "        # 8.1 Apply denoising_end\n",
    "        if (\n",
    "            self.denoising_end is not None\n",
    "            and isinstance(self.denoising_end, float)\n",
    "            and self.denoising_end > 0\n",
    "            and self.denoising_end < 1\n",
    "        ):\n",
    "            discrete_timestep_cutoff = int(\n",
    "                round(\n",
    "                    self.scheduler.config.num_train_timesteps\n",
    "                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)\n",
    "                )\n",
    "            )\n",
    "            num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n",
    "            timesteps = timesteps[:num_inference_steps]\n",
    "\n",
    "        # 9. Optionally get Guidance Scale Embedding\n",
    "        timestep_cond = None\n",
    "        if self.unet.config.time_cond_proj_dim is not None:\n",
    "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "            timestep_cond = self.get_guidance_scale_embedding(\n",
    "                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
    "            ).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "\n",
    "        ##################################\n",
    "        ########### MODIFIED #############\n",
    "        # Initialize\n",
    "        self.unet.GLOBAL_BUFFER.current_step = -1\n",
    "        ########### MODIFIED #############\n",
    "        ##################################\n",
    "        self._num_timesteps = len(timesteps)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
    "                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "                    added_cond_kwargs[\"image_embeds\"] = image_embeds\n",
    "\n",
    "                ##################################\n",
    "                ########### MODIFIED #############\n",
    "                # Before each U-Net forward pass, we increment the step\n",
    "                self.unet.GLOBAL_BUFFER.current_step += 1\n",
    "                ########### MODIFIED #############\n",
    "                ##################################\n",
    "\n",
    "\n",
    "                ##################################\n",
    "                ########### MODIFIED #############\n",
    "                \n",
    "                if self.unet.GLOBAL_BUFFER.score_mix:\n",
    "                    noise_pred = self.score_mix(latent_model_input,\n",
    "                                                t,\n",
    "                                                encoder_hidden_states=prompt_embeds,\n",
    "                                                timestep_cond=timestep_cond,\n",
    "                                                cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                                                added_cond_kwargs=added_cond_kwargs,\n",
    "                                                return_dict=False)\n",
    "                else:\n",
    "                    noise_pred = self.unet(\n",
    "                        latent_model_input,\n",
    "                        t,\n",
    "                        encoder_hidden_states=prompt_embeds,\n",
    "                        timestep_cond=timestep_cond,\n",
    "                        cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                        added_cond_kwargs=added_cond_kwargs,\n",
    "                        return_dict=False,\n",
    "                    )[0]\n",
    "                ########### MODIFIED #############\n",
    "                ##################################\n",
    "\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
    "                    # Based on 3.4. in https://huggingface.co/papers/2305.08891\n",
    "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents_dtype = latents.dtype\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "                if latents.dtype != latents_dtype:\n",
    "                    if torch.backends.mps.is_available():\n",
    "                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                        latents = latents.to(latents_dtype)\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    add_text_embeds = callback_outputs.pop(\"add_text_embeds\", add_text_embeds)\n",
    "                    add_time_ids = callback_outputs.pop(\"add_time_ids\", add_time_ids)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "                if XLA_AVAILABLE:\n",
    "                    xm.mark_step()\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # make sure the VAE is in float32 mode, as it overflows in float16\n",
    "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
    "\n",
    "            if needs_upcasting:\n",
    "                self.upcast_vae()\n",
    "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
    "            elif latents.dtype != self.vae.dtype:\n",
    "                if torch.backends.mps.is_available():\n",
    "                    # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                    self.vae = self.vae.to(latents.dtype)\n",
    "\n",
    "            # unscale/denormalize the latents\n",
    "            # denormalize with the mean and std if available and not None\n",
    "            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n",
    "            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n",
    "            if has_latents_mean and has_latents_std:\n",
    "                latents_mean = (\n",
    "                    torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "                )\n",
    "                latents_std = (\n",
    "                    torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "                )\n",
    "                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n",
    "            else:\n",
    "                latents = latents / self.vae.config.scaling_factor\n",
    "\n",
    "            image = self.vae.decode(latents, return_dict=False)[0]\n",
    "\n",
    "            # cast back to fp16 if needed\n",
    "            if needs_upcasting:\n",
    "                self.vae.to(dtype=torch.float16)\n",
    "        else:\n",
    "            image = latents\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # apply watermark if available\n",
    "            if self.watermark is not None:\n",
    "                image = self.watermark.apply_watermark(image)\n",
    "\n",
    "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusionXLPipelineOutput(images=image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4094d2",
   "metadata": {},
   "source": [
    "이떄, 구현에서 볼 수 있듯 $\\lambda_t가 0 일때는 FR을 적용한 모델에서의 noise prediction이 필요가 없으므로, skip하도록 구현된 것을 확인 할 수 있습니다.\n",
    "\n",
    "```\n",
    "def score_mix(self, model_input, t, **kwargs):\n",
    "    total_step      = self.unet.GLOBAL_BUFFER.total_step\n",
    "    current_step    = self.unet.GLOBAL_BUFFER.current_step\n",
    "    skip_cnt        = self.unet.GLOBAL_BUFFER.skip_cnt\n",
    "\n",
    "    # Compute KeyFrame\n",
    "    if current_step in skip_cnt :\n",
    "        self.jump_score = self.unet(model_input, t, **kwargs)[0]\n",
    "        return self.jump_score\n",
    "    else :\n",
    "        step_ratio = current_step/total_step\n",
    "        bias       = self.unet.GLOBAL_BUFFER.bias\n",
    "        lamb       = self.lamb_schedule(step_ratio, _b=bias)\n",
    "\n",
    "        if DEBUG: print(lamb)\n",
    "        if lamb < 1e-10 :\n",
    "            # Reduced NFE\n",
    "            e_t = self.jump_score.   <------ 0에 가까울때는 계산하지 않고 스킵.\n",
    "        else :\n",
    "            # Score Mixing\n",
    "            noise_pred = self.unet(model_input, t, **kwargs)[0]\n",
    "\n",
    "            if type(noise_pred) == list or type(noise_pred) == tuple :\n",
    "                # Maybe this loop can be little slow..\n",
    "                e_t = [  self.mix_func(lamb, e_t_i, e_jump_i) for e_t_i , e_jump_i in zip(noise_pred, self.jump_score) ]\n",
    "            else :\n",
    "                e_t = self.mix_func(lamb, noise_pred, self.jump_score)\n",
    "        return e_t\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c4006",
   "metadata": {},
   "source": [
    "이러한 Score Mixing을 적용하여 다시 이미지를 생성해 성능을 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9b75167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapping_frdiff(pipe, total_step, skip_interval, bias=0.5, score_mix=True, debug=False):\n",
    "\n",
    "    specials = {\n",
    "    ResnetBlock2D : SkipResnetBlock2D,\n",
    "    Transformer2DModel : SkipTransformer2DModel,\n",
    "    }\n",
    "    global DEBUG\n",
    "    DEBUG=debug\n",
    "\n",
    "    # Set BUFFER\n",
    "    class BUFFER : pass\n",
    "    b = BUFFER()\n",
    "    pipe.GLOBAL_BUFFER = b\n",
    "\n",
    "    # Inject to every layer\n",
    "    for n, m in pipe.unet.named_modules() :\n",
    "        m.module_name = n\n",
    "        m.GLOBAL_BUFFER = b\n",
    "\n",
    "    pipe.unet.GLOBAL_BUFFER.total_step       = total_step\n",
    "    pipe.unet.GLOBAL_BUFFER.skip_interval    = skip_interval\n",
    "    pipe.unet.GLOBAL_BUFFER.bias             = bias\n",
    "    pipe.unet.GLOBAL_BUFFER.score_mix        = score_mix\n",
    "    pipe.unet.GLOBAL_BUFFER.skip_cnt         = list(range(0,total_step,skip_interval))\n",
    "    print(\"[SKIPPING] Buffer Sharing Complete\")\n",
    "\n",
    "    ### Patch Classes\n",
    "    # main\n",
    "    pipe.__class__ = SkipStableDiffusionXLPipeline\n",
    "\n",
    "    # Blocks\n",
    "    for n, m in pipe.unet.named_modules() :\n",
    "        if type(m) in specials.keys():\n",
    "            print(f'Convert {type(m)} to {type(m)}')\n",
    "            m.__class__ = specials[type(m)]\n",
    "\n",
    "            m.update()\n",
    "    print(\"[SKIPPING] Class Patching Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                                 variant=\"fp16\", torch_dtype=torch.float16,\n",
    "                                                 use_safetensors=True)\n",
    "\n",
    "pipe.to(f'cuda')\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "wrapping_frdiff(pipe, total_step=50, skip_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "t1 = time.time()\n",
    "image_fr_and_sm = pipe(prompt, generator=generator, num_inference_steps=num_steps).images[0]\n",
    "t2 = time.time()\n",
    "\n",
    "time_fr_and_sm = t2 - t1\n",
    "print(f\"Feature Reuse + Score Mixing Time : {time_fr_and_sm:.2f} seconds\")\n",
    "image_fr_and_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1883643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(image_ddim)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(f\"DDIM\\nTime: {time_ddim:.2f} seconds\")\n",
    "\n",
    "axes[1].imshow(image_fr)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(f\"FRDiff\\nTime: {time_fr:.2f} seconds\")\n",
    "\n",
    "axes[2].imshow(image_fr_and_sm)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title(f\"FR+SM\\nTime: {time_fr_and_sm:.2f} seconds\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e2211",
   "metadata": {},
   "source": [
    "SM을 적용한 경우, 생성시간이 더욱 빨라지면서도,  원본 이미지와 형태적 특성이 어느정도 align되는, low frequency 성분이 보존되어 최종 생성 품질이 더 좋은 특성을 보임을 알 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84bb3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
